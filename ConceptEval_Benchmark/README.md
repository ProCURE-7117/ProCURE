# ConceptEval_Benchmark
This repository includes a benchmark for evaluating large model programming concept predicates.

## Dataset Origin and Construction

This benchmark is built upon three widely-used programming datasets: HumanEval, MBPP, and CodeContests. For each original code sample, we design and apply five types of program concept predicate-based counterfactual perturbations to systematically generate counterfactual code data. Each data entry includes:

- **origin_code**: The original code snippet.
- **Five types of counterfactual codes**, corresponding to:
  - ifelse_flip_code
  - independent_swap_code
  - def_use_code
  - variable_name_random_code
  - variable_name_shuttle_code

For each perturbation, if it can be applied to the current code, the signal is set to 1 and the perturbed code is provided; otherwise, the signal is 0 and the code is an empty string.

Example:
```json
{
  "task_id": "HumanEval/14",
  "origin_code": "...",
  "ifelse_flip_code": {"signal": 0, "code": ""},
  "independent_swap_code": {"signal": 0, "code": ""},
  "def_use_code": {"signal": 1, "code": "..."},
  "variable_name_random_code": {"signal": 1, "code": "..."},
  "variable_name_shuttle_code": {"signal": 1, "code": "..."}
}
```

In this way, we systematically generate multiple counterfactual versions for each original sample, enabling comprehensive evaluation of large models' understanding and generalization of programming concepts.

## Benchmark Overview and Mutation Statistics

This benchmark evaluates the robustness and generalization of large language models to program concept perturbations. For each dataset, we report the number of successful counterfactual mutations for each perturbation type:

- **A**: ifelse_flip_code
- **B**: independent_swap_code
- **C**: def_use_code
- **D**: variable_name_random_code
- **E**: variable_name_shuttle_code

### HumanEval Dataset
Mutation successes: A = 24, B = 37, C = 144, D = 141, E = 145

### MBPP Dataset
Mutation successes: A = 198, B = 179, C = 957, D = 874, E = 906

### CodeContests Dataset
Mutation successes: A = 3776, B = 4895, C = 6980, D = 7183, E = 6764

These statistics reflect the number of original code samples in each dataset for which each type of counterfactual perturbation was successfully applied. This provides a quantitative basis for evaluating model performance on diverse and systematically generated counterfactuals.

## Evaluation Protocol

Our benchmark employs two complementary evaluation metrics:

### 1. Pass@k
To evaluate general task performance, we adopt the standard **Pass@k** metric ([Abdin et al., 2024](https://arxiv.org/abs/2403.05530); [Luo et al., 2023](https://arxiv.org/abs/2306.11644); [Wei et al., 2023](https://arxiv.org/abs/2306.07971)). Given $k$ generated outputs $\{y_i^1, \dots, y_i^k\}$ for a problem $x_i$, Pass@k computes the fraction of $n$ problems where at least one output satisfies the instruction:

$$
\text{Pass@}k = \frac{1}{n} \sum_{i=1}^{n} \left( 1 - \prod_{j=1}^{k} (1 - A(h_i, x_i, y_i^j)) \right)
$$

In practice, we adopt the unbiased estimator from [Chen et al., 2021](https://arxiv.org/abs/2107.03374) to approximate per-sample Pass@k as:

$$
1 - \frac{\binom{m - c}{k}}{\binom{m}{k}}
$$

where $m$ is the number of generated samples and $c \leq m$ denotes the number of correct ones.

### 2. Concept Consistency Score (CCS)
**CCS** measures a model's ability to understand programming concept predicates ([Hooda et al., 2024](https://arxiv.org/abs/2402.13908)). Given a set of original–counterfactual program pairs $(x, x^{\text{cf}})$ under a shared instruction $h$, we define consistency as:

$$
\mathrm{Consistent}(h, x, x^{\text{cf}}) = \mathbb{I} \left[ A(h, x, M(h, x)) = A(h, x^{\text{cf}}, M(h, x^{\text{cf}})) \right]
$$

where $A$ is the evaluation function (e.g., test case checker), $M$ is the model, and $\mathbb{I}[\cdot]$ is the indicator function. CCS is then computed as the average consistency over all such pairs.

These metrics together provide a comprehensive assessment of both general problem-solving ability and conceptual robustness to counterfactual perturbations.

## Counterfactual Benchmarks and Input Format

This repository provides three counterfactual programming concept predicate benchmarks:
- `humaneval_counter.jsonl`
- `mbpp_counter.jsonl`
- `codecontests_counter.jsonl`

Each file contains counterfactual variants for the respective dataset, generated by applying five types of program concept predicate perturbations.

## Usage

This benchmark supports two evaluation metrics: **pass@k** and **Concept Consistency Score (CCS)**. You can select the metric using the `--metric` parameter.

### Input File Format
- For **pass@k**: Each line in your input JSONL file should contain at least `task_id` and `completion`.
- For **CCS**: Each line should contain `task_id`, `completion`, and `mutation_id` (where `mutation_id` is 0 for the original and 1–5 for the five perturbations).

**Example entry for CCS:**
```json
{
  "task_id": "HumanEval/14",
  "completion": "def ...",  // model's solution
  "mutation_id": 3           // 1-5 for perturbations, 0 for original
}
```

### Evaluating pass@k
To evaluate pass@k on any of the three counterfactual benchmarks, run:

```bash
python evaluate.py --sample_file <your_model_outputs.jsonl> --dataset <humaneval|mbpp|codecontests> --k 1,5 --metric pass@k
```

- `--sample_file`: Path to your model's generated samples (JSONL format, with `task_id` and `completion` fields).
- `--dataset`: Choose from `humaneval`, `mbpp`, or `codecontests`. This will select the correct counterfactual benchmark file from the current directory (e.g., `humaneval_counter.jsonl`).
- `--k`: Comma-separated list of k values for pass@k (default: 1,5).
- `--metric`: Set to `pass@k` for standard pass@k evaluation (default).

**Example:**
```bash
python evaluate.py --sample_file my_humaneval_counter_outputs.jsonl --dataset humaneval --k 1,5 --metric pass@k
python evaluate.py --sample_file my_mbpp_counter_outputs.jsonl --dataset mbpp --k 1,5 --metric pass@k
python evaluate.py --sample_file my_codecontests_counter_outputs.jsonl --dataset codecontests --k 1,5 --metric pass@k
```

### Evaluating Concept Consistency Score (CCS)
To evaluate CCS, your input file must include the `mutation_id` field. Run:

```bash
python evaluate.py --sample_file <your_model_outputs.jsonl> --dataset <humaneval|mbpp|codecontests> --metric ccs
```

- `--sample_file`: Path to your model's generated samples (JSONL format, with `task_id`, `completion`, and `mutation_id` fields).
- `--dataset`: Choose from `humaneval`, `mbpp`, or `codecontests`.
- `--metric`: Set to `ccs` for Concept Consistency Score evaluation.

**Example:**
```bash
python evaluate.py --sample_file my_humaneval_counter_outputs.jsonl --dataset humaneval --metric ccs
python evaluate.py --sample_file my_mbpp_counter_outputs.jsonl --dataset mbpp --metric ccs
python evaluate.py --sample_file my_codecontests_counter_outputs.jsonl --dataset codecontests --metric ccs
```

### Output
- For **pass@k**, the script prints the pass@k values for the specified k.
- For **CCS**, the script prints the CCS value, the number of consistent pairs, and the total number of valid pairs.

---

**Notes:**
- The `mutation_id` should be 1–5 for the five perturbations, and 0 for the original code if included.
- For pass@k, the evaluation only uses the original or perturbed completions as independent samples.
- For CCS, the evaluation compares the model's outputs on original and counterfactual pairs sharing the same `task_id` and instruction, grouped by `mutation_id`.
- Make sure your input files are in the correct format for the metric you wish to evaluate.


